---
title: "caret_vignette"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{caret_vignette}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>"
)
```

```{r setup}

library(BonusLab)
ridgereg <- getFromNamespace("ridgereg", "BonusLab")

```

Use the caret package and the 'ridgereg()' function to create a predictive model for the [BostonHousing] data found in the [mlbench] package. 


## caret example

Divide the BostonHousing data into a test and training dataset using the caret package.
```{r}

data(BostonHousing, package="mlbench")

train_indices <- caret::createDataPartition(BostonHousing$crim, times = 1, p = 0.8, list = FALSE, groups = 2)

# create training set
train_data <- BostonHousing[train_indices,]

# create testing set
test_data  <- BostonHousing[-train_indices,]

```

Fit a linear regression model and fit a linear regression model with forward selection of covariates on the training dataset.

```{r}

# Fit a linear regression model on the training dataset
saturated_model <- lm(medv ~ ., data = train_data)
summary(saturated_model)


# Fit a linear regression model with forward selection on the training dataset
null_model <- lm(medv ~ 1, data = train_data)

lm_forward_model <- MASS::stepAIC(null_model, direction = "forward",
                            scope = list(lower = null_model, 
                                         upper = saturated_model), trace = FALSE)
summary(lm_forward_model)

```


Evaluate the performance of this model on the training dataset.
```{r}

broom::glance(saturated_model) %>%
  dplyr::select(r.squared, adj.r.squared, sigma, AIC, BIC, p.value)
broom::glance(lm_forward_model) %>%
  dplyr::select(r.squared, adj.r.squared, sigma, AIC, BIC, p.value)

sum((round(predict(saturated_model,train_data),1)-train_data$medv)^2)
sum((round(predict(lm_forward_model,train_data),1)-train_data$medv)^2)

```


Fit a ridge regression model using your ridgereg() function to the training dataset for different values of lambda. 
```{r}

ridgereg_model_lambda0_01 <- ridgereg(medv ~ .,data=train_data,lambda=0.01)
ridgereg_model_lambda0_02 <- ridgereg(medv ~ .,data=train_data,lambda=0.02)
ridgereg_model_lambda0_05 <- ridgereg(medv ~ .,data=train_data,lambda=0.05)
ridgereg_model_lambda0_1 <- ridgereg(medv ~ .,data=train_data,lambda=0.1)

sum((round(ridgereg_model_lambda0_01$pred(),1)-train_data$medv)^2)
sum((round(ridgereg_model_lambda0_02$pred(),1)-train_data$medv)^2)
sum((round(ridgereg_model_lambda0_05$pred(),1)-train_data$medv)^2)
sum((round(ridgereg_model_lambda0_1$pred(),1)-train_data$medv)^2)

```


Find the best hyperparameter value for lambda using 10-fold cross-validation on the training set. 
```{r}

kfolds <- 10
train_n <- nrow(train_data)
folds_index <- caret::createFolds(train_data$crim, 
                                  k = 10, list = TRUE, returnTrain = TRUE)
cv_err <- rep(0, kfolds)

for(lambda in seq(0,0.1, by=0.01)){
  for(i in 1:kfolds){
  in_data <- train_data[as.vector(folds_index[[i]]),]
  out_data <- train_data[-as.vector(folds_index[[1]]),]

  fit <- ridgereg(medv ~ ., in_data, lambda)
  preds <- fit$predict(fit$formula, out_data)

  err <- out_data$medv - preds
  mse <- mean(err^2)
  # Record the RMSE
  cv_err[i] <- sqrt(mse)
  }
}

cv_err # lambda = 0.02 is best



```


Evaluate the performance of all three models on the test dataset and write some concluding comments.
```{r}

sum((round(predict(saturated_model,test_data),1)-test_data$medv)^2)
sum((round(predict(lm_forward_model,test_data),1)-test_data$medv)^2)

fit <- ridgereg(formula(lm_forward_model), train_data, 0.02)
preds <- fit$predict(fit$formula, test_data)

sum((round(preds,1)-test_data$medv)^2)

```
In my case the normal linear model with forward selection of the included coefficients has the lowest sum of squared differences between the predicted values and the real values. This means it is the best. 

